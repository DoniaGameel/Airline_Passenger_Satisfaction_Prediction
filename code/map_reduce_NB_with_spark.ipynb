{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pyspark.sql import SparkSession\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Naive Bayes Classifier\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "current_dir = os.getcwd() \n",
    "relative_path_train = os.path.join('..', 'data', 'preprocessed_train_data_after_grouping.csv')\n",
    "relative_path_test = os.path.join('..', 'data', 'preprocessed_test_data_after_grouping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into Spark DataFrames\n",
    "train_df = spark.read.csv(relative_path_train, header=True, inferSchema=True)\n",
    "test_df = spark.read.csv(relative_path_test, header=True, inferSchema=True)\n",
    "\n",
    "# Drop the target column and create feature vectors\n",
    "feature_columns = [col for col in train_df.columns if col != \"satisfaction\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "train_features = assembler.transform(train_df).select(\"features\", \"satisfaction\")\n",
    "test_features = assembler.transform(test_df).select(\"features\", \"satisfaction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the label column from train_features DataFrame\n",
    "train_labels_list = train_features.select(\"satisfaction\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Extract the label column from test_features DataFrame\n",
    "test_labels_list = test_features.select(\"satisfaction\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MinMaxScaler\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "# Fit scaler to train data and transform train and test data\n",
    "scaler_model = scaler.fit(train_features)\n",
    "train_scaled = scaler_model.transform(train_features)\n",
    "test_scaled = scaler_model.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate prior probabilities\n",
    "def mapper_prior_probability(data):\n",
    "    class_counts = data.groupBy(\"satisfaction\").count().collect()\n",
    "    return {\"class_counts\": {row[\"satisfaction\"]: row[\"count\"] for row in class_counts}}\n",
    "\n",
    "def reducer_prior_probability(mapped_results):\n",
    "    class_counts_total = {}\n",
    "    for class_counts in mapped_results.values():\n",
    "        for label, count in class_counts.items():\n",
    "            if label not in class_counts_total:\n",
    "                class_counts_total[label] = count\n",
    "            else:\n",
    "                class_counts_total[label] += count\n",
    "\n",
    "    total_samples = sum(class_counts_total.values())\n",
    "    class_probabilities_prior = {label: count / total_samples for label, count in class_counts_total.items()}\n",
    "\n",
    "    return class_probabilities_prior\n",
    "\n",
    "mapped_results_prior = mapper_prior_probability(train_scaled)\n",
    "class_probabilities_prior = reducer_prior_probability(mapped_results_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate feature counts\n",
    "def mapper(data):\n",
    "    feature_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "    for row in data:\n",
    "        label = row[\"satisfaction\"]  # Assuming the label column is named \"satisfaction\"\n",
    "        features = row[\"features\"]  # Keep as Spark DataFrame column\n",
    "        for idx, value in zip(features.indices, features.values):  # Iterate over non-zero elements\n",
    "            feature_name = f\"feature_{idx}\"\n",
    "            feature_value_str = str(value)\n",
    "            feature_counts[label][feature_name][feature_value_str] += 1\n",
    "    return [feature_counts]\n",
    "\n",
    "def reducer(mapped_results):\n",
    "    feature_counts = {}\n",
    "    \n",
    "    for f_count in mapped_results:\n",
    "        for key, count in f_count.items():\n",
    "            if key[0] not in feature_counts:\n",
    "                feature_counts[key[0]] = {}\n",
    "            if key[1] not in feature_counts[key[0]]:\n",
    "                feature_counts[key[0]][key[1]] = {}\n",
    "            feature_counts[key[0]][key[1]][key[2]] = count + feature_counts.get(key[0], {}).get(key[1], {}).get(key[2], 0)  # Handle missing values in nested dictionaries\n",
    "    return feature_counts\n",
    "\n",
    "feature_counts_rdd = train_scaled.rdd.flatMap(mapper) \\\n",
    "                                     .reduceByKey(reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate features probabilities\n",
    "def mapper_train_naive_bayes_feature_probabilities(feature_counts):\n",
    "    partial_feature_probabilities = []\n",
    "\n",
    "    for label, features in feature_counts.items():\n",
    "        partial_feature_probabilities.append((label, features))\n",
    "\n",
    "    return partial_feature_probabilities\n",
    "\n",
    "\n",
    "def reducer_train_naive_bayes_feature_probabilities(mapped_results):\n",
    "    feature_probabilities = {}\n",
    "\n",
    "    for label, features in mapped_results:\n",
    "        # Calculate feature probabilities\n",
    "        if label not in feature_probabilities:\n",
    "            feature_probabilities[label] = {}\n",
    "        for feature, values in features.items():\n",
    "            total_feature_count = sum(values.values())\n",
    "            if feature not in feature_probabilities[label]:\n",
    "                feature_probabilities[label][feature] = {}\n",
    "            for value, count in values.items():\n",
    "                if value not in feature_probabilities[label][feature]:\n",
    "                    feature_probabilities[label][feature][value] = count / total_feature_count\n",
    "                else:\n",
    "                    feature_probabilities[label][feature][value] += count / total_feature_count\n",
    "\n",
    "    return feature_probabilities\n",
    "\n",
    "feature_probabilities_rdd = feature_counts_rdd.mapPartitions(\n",
    "    lambda partition: mapper_train_naive_bayes_feature_probabilities(next(partition))\n",
    ") \\\n",
    ".reduceByKey(reducer_train_naive_bayes_feature_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_probabilities_dict = feature_probabilities_rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "calculates the probability of each class for a given sample \n",
    "'''\n",
    "def mapper_predict_naive_bayes(class_probabilities, feature_probabilities_rdd, sample):\n",
    "    log_probs = {}\n",
    "\n",
    "    for label, class_prob in class_probabilities.items():\n",
    "        log_prob = math.log(class_prob)\n",
    "        for feature, value in sample.items():\n",
    "            # Retrieve feature probabilities from the RDD\n",
    "            feature_prob = feature_probabilities_rdd.filter(lambda x: x[0] == label) \\\n",
    "                                                     .map(lambda x: x[1]) \\\n",
    "                                                     .flatMap(lambda x: x.get(feature, {}).items()) \\\n",
    "                                                     .filter(lambda x: x[0] == value) \\\n",
    "                                                     .map(lambda x: x[1]) \\\n",
    "                                                     .reduce(lambda a, b: a if a > b else b, default=0)\n",
    "            log_prob += math.log(feature_prob)\n",
    "\n",
    "        log_probs[label] = log_prob\n",
    "\n",
    "    return log_probs\n",
    "\n",
    "'''\n",
    "selects the class with the highest probability as the predicted class. \n",
    "'''\n",
    "def reducer_predict_naive_bayes(mapped_results):\n",
    "    predicted_class = None\n",
    "    max_log_prob = float('-inf')\n",
    "\n",
    "    for label, log_prob in mapped_results.items():\n",
    "        if log_prob > max_log_prob:\n",
    "            max_log_prob = log_prob\n",
    "            predicted_class = label\n",
    "\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_partition(iterator):\n",
    "    for sample in iterator:\n",
    "        log_probs = mapper_predict_naive_bayes(class_probabilities_prior, feature_probabilities_rdd, sample)\n",
    "        yield reducer_predict_naive_bayes(log_probs)\n",
    "\n",
    "# Make predictions\n",
    "predictions_rdd = test_scaled.rdd.map(lambda row: row.asDict()) \\\n",
    "    .mapPartitions(predict_partition)\n",
    "\n",
    "train_predictions_rdd = train_scaled.rdd.map(lambda row: row.asDict()) \\\n",
    "    .mapPartitions(predict_partition)\n",
    "\n",
    "# Collect predictions into a list\n",
    "predictions = predictions_rdd.collect()\n",
    "train_predictions = train_predictions_rdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(test_labels_list, predictions)\n",
    "f1 = f1_score(test_labels_list, predictions)\n",
    "precision = precision_score(test_labels_list, predictions)\n",
    "recall = recall_score(test_labels_list, predictions)\n",
    "auc_score = roc_auc_score(test_labels_list, predictions)\n",
    "\n",
    "train_accuracy = accuracy_score(train_labels_list, train_predictions)\n",
    "\n",
    "print(\"Balanced Accuracy: \", auc_score)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Testing Accuracy:\", accuracy)\n",
    "print(\"f1_score:\", f1)\n",
    "print(\"precision:\", precision)\n",
    "print(\"recall:\", recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
