{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m358.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:21\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m320.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=9d441740999c28ad01f6e075de071df3129d0117f5e83bb7e89c3c74393178ae\n",
      "  Stored in directory: /home/donia/.cache/pip/wheels/da/78/6d/54350e0243f65f77dccf6ebe2ed5559faf6900559e904fb957\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load The Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd() \n",
    "relative_path_train = os.path.join('..', 'data', 'preprocessed_train_data.csv')\n",
    "relative_path_test = os.path.join('..', 'data', 'preprocessed_test_data.csv')\n",
    "\n",
    "preprocessed_train_data = pd.read_csv(os.path.join(current_dir, relative_path_train))\n",
    "preprocessed_test_data = pd.read_csv(os.path.join(current_dir, relative_path_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = preprocessed_train_data.drop([\"satisfaction\"], axis = 1)\n",
    "y_train = preprocessed_train_data[\"satisfaction\"]\n",
    "\n",
    "x_test = preprocessed_test_data.drop([\"satisfaction\"], axis = 1)\n",
    "y_test = preprocessed_test_data[\"satisfaction\"]\n",
    "\n",
    "#y_test = y_test.replace({\"satisfied\":1, \"neutral or dissatisfied\":0})\n",
    "\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scale The Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(preprocessed_train_data)\n",
    "test_scaled = scaler.transform(preprocessed_test_data)\n",
    "\n",
    "# Convert scaled arrays back to DataFrame\n",
    "train_scaled_df = pd.DataFrame(train_scaled, columns=preprocessed_train_data.columns)\n",
    "test_scaled_df = pd.DataFrame(test_scaled, columns=preprocessed_test_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prior Probabilities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior Probability mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_prior_probability(data):\n",
    "    class_counts = {}\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        label = row[\"satisfaction\"]\n",
    "        if label not in class_counts:\n",
    "            class_counts[label] = 1\n",
    "        else:\n",
    "            class_counts[label] += 1\n",
    "\n",
    "    return [(class_counts, {})]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior Probability reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer_prior_probability(mapped_results):\n",
    "    class_counts_total = {}\n",
    "\n",
    "    for c_count, _ in mapped_results:\n",
    "        for label, count in c_count.items():\n",
    "            if label not in class_counts_total:\n",
    "                class_counts_total[label] = count\n",
    "            else:\n",
    "                class_counts_total[label] += count\n",
    "\n",
    "    total_samples = sum(class_counts_total.values())\n",
    "    class_probabilities = {label: count / total_samples for label, count in class_counts_total.items()}\n",
    "\n",
    "    return class_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features Count**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features count mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The mapper iterates over each row in the training data \n",
    "and counts occurrences of each feature-value pair for each class.\n",
    "'''\n",
    "def mapper(data):\n",
    "    feature_counts = {}\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        label = row[\"satisfaction\"]\n",
    "        for feature_name, feature_value in row.items():\n",
    "            if feature_name != \"satisfaction\":\n",
    "                key = (label, feature_name, feature_value)\n",
    "                if key not in feature_counts:\n",
    "                    feature_counts[key] = 1\n",
    "                else:\n",
    "                    feature_counts[key] += 1\n",
    "                    \n",
    "    return [feature_counts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features count Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "aggregates the counts from all mappers.\n",
    "sum up the counts for each class and each feature-value pair.\n",
    "'''\n",
    "def reducer(mapped_results):\n",
    "    '''\n",
    "    the outer keys represent classes, the inner keys represent features, and the innermost keys represent feature values. \n",
    "    '''\n",
    "    feature_counts = {}\n",
    "\n",
    "    for f_count in mapped_results:\n",
    "        for key, count in f_count.items():\n",
    "            if key[0] not in feature_counts:\n",
    "                feature_counts[key[0]] = {}\n",
    "            if key[1] not in feature_counts[key[0]]:\n",
    "                feature_counts[key[0]][key[1]] = {}\n",
    "            feature_counts[key[0]][key[1]][key[2]] = count\n",
    "            \n",
    "    return feature_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Naive Bayes Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_train_naive_bayes_feature_probabilities(feature_counts):\n",
    "    '''\n",
    "    It transforms the feature_counts dictionary into a list of tuples, \n",
    "    where each tuple contains a class label and the associated features.\n",
    "    '''\n",
    "    partial_feature_probabilities = []\n",
    "\n",
    "    for label, features in feature_counts.items():\n",
    "        partial_feature_probabilities.append((label, features))\n",
    "\n",
    "    return partial_feature_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer_train_naive_bayes_feature_probabilities(mapped_results):\n",
    "    feature_probabilities = {}\n",
    "\n",
    "    for label, features in mapped_results:\n",
    "        # Calculate feature probabilities\n",
    "        if label not in feature_probabilities:\n",
    "            feature_probabilities[label] = {}\n",
    "        for feature, values in features.items():\n",
    "            total_feature_count = sum(values.values())\n",
    "            if feature not in feature_probabilities[label]:\n",
    "                feature_probabilities[label][feature] = {}\n",
    "            for value, count in values.items():\n",
    "                if value not in feature_probabilities[label][feature]:\n",
    "                    feature_probabilities[label][feature][value] = count / total_feature_count\n",
    "                else:\n",
    "                    feature_probabilities[label][feature][value] += count / total_feature_count\n",
    "\n",
    "    return feature_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "calculates the probability of each class for a given sample \n",
    "'''\n",
    "def mapper_predict_naive_bayes(class_probabilities, feature_probabilities, sample):\n",
    "    log_probs = {}\n",
    "\n",
    "    for label, class_prob in class_probabilities.items():\n",
    "        log_prob = math.log(class_prob)\n",
    "        for feature, value in sample.items():\n",
    "            if feature in feature_probabilities[label] and value in feature_probabilities[label][feature]:\n",
    "                log_prob += math.log(feature_probabilities[label][feature][value])\n",
    "            else:\n",
    "                # Handle unseen feature values\n",
    "                log_prob += 0\n",
    "        \n",
    "        log_probs[label] = log_prob\n",
    "\n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "selects the class with the highest probability as the predicted class. \n",
    "'''\n",
    "def reducer_predict_naive_bayes(mapped_results):\n",
    "    predicted_class = None\n",
    "    max_log_prob = float('-inf')\n",
    "\n",
    "    for label, log_prob in mapped_results.items():\n",
    "        if log_prob > max_log_prob:\n",
    "            max_log_prob = log_prob\n",
    "            predicted_class = label\n",
    "\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Map Reducer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy:  0.7650422898817919\n",
      "Training Accuracy: 0.8885220973206036\n",
      "Testing Accuracy: 0.7626516019436653\n",
      "f1_score: 0.7399809573271012\n",
      "precision: 0.7018307199737296\n",
      "recall: 0.7825171624713959\n"
     ]
    }
   ],
   "source": [
    "# Mapping for Prior Probability\n",
    "mapped_results_prior = mapper_prior_probability(train_scaled_df)\n",
    "class_probabilities_prior = reducer_prior_probability(mapped_results_prior)\n",
    "\n",
    "# Mapping for Feature Counts\n",
    "mapped_results_feature_counts = mapper(train_scaled_df)\n",
    "final_feature_counts = reducer(mapped_results_feature_counts)\n",
    "\n",
    "# Apply map-reduce for Training Naive Bayes Classifier (Feature Probabilities)\n",
    "mapped_results_train_nb = mapper_train_naive_bayes_feature_probabilities(final_feature_counts)\n",
    "feature_probabilities = reducer_train_naive_bayes_feature_probabilities(mapped_results_train_nb)\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "for index, row in test_scaled_df.iterrows():\n",
    "    sample = row.drop(\"satisfaction\").to_dict()\n",
    "    log_probs = mapper_predict_naive_bayes(class_probabilities_prior, feature_probabilities, sample)\n",
    "    predicted_class = reducer_predict_naive_bayes(log_probs)\n",
    "    predictions.append(predicted_class)\n",
    "\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "auc_score = roc_auc_score(y_test, predictions)\n",
    "\n",
    "\n",
    "# Make predictions on the training dataset\n",
    "train_predictions = []\n",
    "for index, row in train_scaled_df.iterrows():\n",
    "    sample = row.drop(\"satisfaction\").to_dict()\n",
    "    log_probs = mapper_predict_naive_bayes(class_probabilities_prior, feature_probabilities, sample)\n",
    "    predicted_class = reducer_predict_naive_bayes(log_probs)\n",
    "    train_predictions.append(predicted_class)\n",
    "\n",
    "# Calculate training accuracy\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "print(\"Balanced Accuracy: \", auc_score)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Testing Accuracy:\", accuracy)\n",
    "print(\"f1_score:\", f1)\n",
    "print(\"precision:\", precision)\n",
    "print(\"recall:\", recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geospatial)",
   "language": "python",
   "name": "geospatial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
